/*

active-learning-scala: Active Learning library for Scala
Copyright (c) 2014 Davi Pereira dos Santos

   This program is free software: you can redistribute it and/or modify
   it under the terms of the GNU General Public License as published by
   the Free Software Foundation, either version 3 of the License, or
   (at your option) any later version.

   This program is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
   GNU General Public License for more details.

   You should have received a copy of the GNU General Public License
   along with this program.  If not, see <http://www.gnu.org/licenses/>.
*/

package clean.tex

import java.io.{File, FileWriter, OutputStream, PrintStream}

import clean.lib.{FilterTrait, Log, Rank}
import clus.Clus
import ml.Pattern
import ml.classifiers.{SVMLibRBF, Learner, NinteraELM, RF}
import ml.models.ELMModel
import org.apache.commons.math3.stat.correlation.SpearmansCorrelation
import util.{Datasets, Stat}
import weka.attributeSelection.{BestFirst, AttributeSelection, WrapperSubsetEval, GreedyStepwise}
import weka.classifiers.`lazy`.IBk
import weka.classifiers.trees.RandomForest
import weka.core.DenseInstance
import weka.core.converters.ArffSaver

import scala.collection.mutable
import scala.util.Random

trait MetaTrait extends FilterTrait with Rank with Log {
  def clusSettings(natts: Int, targets: Int, seed: Int, arqtr: String, arqts: String) = {
    val ultimoDesc = natts + 1
    val primeiroTarget = ultimoDesc + 1
    val ultimoTarget = primeiroTarget + targets - 1
    Seq(
      "[Data]",
      s"File = $arqtr.arff",
      s"TestSet = $arqts.arff",
      "",
      "[General]",
      s"RandomSeed = $seed",
      "",
      "[Attributes]",
      s"Descriptive = 2-$ultimoDesc",
      s"Target = $primeiroTarget-$ultimoTarget",
      s"Clustering = $primeiroTarget-$ultimoTarget",
      "Weights = 1",
      "",
      "[Tree]",
      "Heuristic = VarianceReduction",
      "FTest = 1",
      //      "PruningMethod = C4.5",//quebra
      //            "PruningMethod = M5Multi",//ajuda
      //      "M5PruningMult = 1", //ajuda mais
      "",
      "[Ensemble]",
      "Iterations = 500",
      "EnsembleMethod = Bagging", //Bagging, RForest, RSubspaces, BagSubspaces só funfou bagging
      "",
      "[Output]",
      "WritePredictions = {Test}"
    ).mkString("\n")
  }


  def arff(strats: String, exemplos: Seq[(Seq[(String, String, String)], String)], print: Boolean = false, context: String, rank: Boolean) = {
    lazy val labels = exemplos.map(x => x._2).distinct.sorted
    lazy val classAtt = "@attribute class {" + labels.map(x => "\"" + x + "\"").mkString(",") + "}"
    val attsNameType = exemplos.head._1 map { case (no, va, ty) => s"$no $ty" }
    val header = List(s"@relation autogeneratedby$context") ++ (attsNameType map (nt => s"@attribute $nt")) ++ (if (rank) List() else List(classAtt))
    val data = exemplos.map(x => "\"" + x._1.head._2 + "\"," + x._1.tail.map(_._2).mkString(",") + (if (rank) "" else s"," + "\"" + x._2 + "\""))
    if (print) {
      //      println("labels: " + labels.mkString(" "))
      //      println(s"#classes: ${labels.size}")
      println(s"#atributos: ${exemplos.head._1.size}")
      println(s"#exemplos: ${exemplos.size}")
    }
    Seq("%" + strats) ++ header ++ Seq("@data") ++ data
  }

  def grava(arq: String, linhas: Seq[String], print: Boolean = false): Unit = {
    val fw = new FileWriter(arq)
    linhas foreach (x => fw.write(s"$x\n"))
    fw.close()
    if (print) linhas foreach println
  }

  def instances2file(patterns: Vector[Pattern], arq: String) {
    val as = new ArffSaver()
    as.setInstances(Datasets.patterns2instances(patterns))
    as.setFile(new File(arq + ".arff"))
    as.writeBatch()
  }

  val originalStream = System.out
  val dummyStream = new PrintStream(new OutputStream() {
    def write(b: Int) {}
  })

  def cv(attsel: Boolean, patterns: Vector[Pattern], leas: Vector[Pattern] => Vector[Learner], rank: Boolean, rs: Int, ks: Int) = {
    (1 to rs).par map { run =>
      val shuffled = new Random(run).shuffle(patterns)
      val bags = shuffled.groupBy(_.base).values.toVector

      Datasets.kfoldCV2(bags, ks, parallel = true) { (trbags, tsbags, fold, minSize) =>
        val seed = run * 100 + fold

        //attsel
        val (tr0, ts0) = trbags.flatten.toVector -> tsbags.flatten.toVector

        val (tr, ts) = if (rank) tr0 -> ts0
        else {
          val f = Datasets.removeBagFilter(tr0)
          val (tr00, ts00) = Datasets.applyFilter(f)(tr0) -> Datasets.applyFilter(f)(ts0)
          tr00 -> ts00
        }

        lazy val (trf, tsf) = replacemissingNom2binRmuselessZscore(tr, ts)
        //pega apenas um ex. por base (um que tiver label mais frequente)
        lazy val tr_trfSemParecidos = Seq(tr, trf) map { trx =>
          if (!rank) tr0.zip(trx).groupBy(_._1.base).map { case (k, lists) =>
            val (_, list) = lists.unzip
            val moda = list.groupBy(_.label).toList.sortBy(_._2.size).last._1
            val attsMedio = media(list.map(_.array))
            val pa = list.head
            val id = pa.id
            val inst = new DenseInstance(1d, attsMedio :+ moda)
            inst.setDataset(pa.dataset)
            Pattern(id, inst, missed = false, pa.parent)
          }.toSeq
          else tr0.zip(trx).groupBy(_._1.base).map { case (k, lists) =>
            val (_, list) = lists.unzip
            val rankMedio = media(list.map(_.targets))
            val descMedio = media(list.map(_.array))
            val pa = list.head
            val id = pa.id
            val inst = new DenseInstance(1d, pa.toDoubleArray.take(1) ++ descMedio ++ rankMedio)
            inst.setDataset(pa.dataset)
            Pattern(id, inst, missed = false, pa.parent)
          }.toSeq
        }
        lazy val (trSemParecidos1, trfSemParecidos1) = tr_trfSemParecidos.head.toVector -> tr_trfSemParecidos(1).toVector

        if (leas(tr).isEmpty) {
          //ELM
          val l = NinteraELM(seed)
          val m0 = l.batchBuild(trfSemParecidos1).asInstanceOf[ELMModel] //selecionar com todos foi pior (e bem mais lento) que tirando similares 38.6 < 44.0
          val L = l.LForMeta(m0, LOO = false)
          val mfull0 = l.batchBuild(trf).asInstanceOf[ELMModel] //treinar com todos foi melhor que tirando similares 44.0 > 42.5
          val mfull = l.fullBuildForMeta(L, mfull0)
          val ELMRanks = tsf.toVector map { p => mfull.output(p) }

          //clus; seed tb serve pra situar run e fold durante paralelização
          val arqtr = s"/run/shm/tr$seed"
          val arqts = s"/run/shm/ts$seed"
          instances2file(trSemParecidos1, arqtr) //sem redundantes: 48/54; com todos 43/44
          instances2file(ts, arqts)
          val f = new FileWriter(s"/run/shm/clus$seed.s")
          f.write(clusSettings(patterns.head.nattributes, patterns.head.nclasses, seed, arqtr, arqts))
          f.close()

          System.setOut(dummyStream)
          Clus.main(Array("-forest", "-silent", s"/run/shm/clus$seed"))
          System.setOut(originalStream)

          val clusPredictionsARFF = Datasets.arff(s"/run/shm/clus$seed.test.pred.arff", dedup = false, rmuseless = false) match {
            case Right(x) => x
            case Left(m) => error(s"${m} <- m")
          }

          val clusOrigRanks_clusPrunRanks = Vector("Original-p") map { str =>
            //          val clusOrigRanks_clusPrunRanks = Vector("Original-p", "Pruned-p") map { str =>
            clusPredictionsARFF.map { pa =>
              pa.array.zipWithIndex.flatMap { case (v, i) => if (pa.attribute(i).name.startsWith(str)) Some(v) else None }
            }
          }

          val trtargets = tr.toSeq map (_.targets)
          val tstargets = ts.toSeq map (_.targets)

          //Default
          val rankMedio = media(trtargets)
          val defaultRanks = ts map (_ => rankMedio)
          def fo(x: Double) = "%2.1f".format(x)

          (clusOrigRanks_clusPrunRanks ++ Vector(ELMRanks, defaultRanks)).zip(Vector("PCT", "ELM", "def")) map { case (ranks, alg) =>
            val spearsTrTs = Seq(trtargets, tstargets).map { txtargets =>
              val speaPorComb = mutable.Queue[(String, String, Double)]()
              ranks.zip(txtargets) foreach { case (ranking, targets) =>
                val r = try {
                  val res = new SpearmansCorrelation().correlation(ranking, targets)
                  if (res.isNaN) 0d
                  //justQuit("\n\n\nNaN no Spear:" + ranking.toList + " " + targets.toList + "\n\n\n")
                  else res
                } catch {
                  case x: Throwable => error("\n " + ranking.toList + "\n " + rankMedio.toList + "\n " + targets.toList + " \n" + x)
                }
                val (min, max) = ranking.min -> ranking.max
                speaPorComb += ((targets.map(_.toInt).mkString(" "), (ranking.map(x => patterns.head.nclasses * (x - min) / (max - min)) map fo).mkString(" "), r))
              }
              speaPorComb
            }
            Resultado(alg, spearsTrTs.head, spearsTrTs(1))
          }

        } else {
          val (trfs, trffs, tsfs, tsffs, trfSemParecidos1fs) = if (attsel) {
            val att = new AttributeSelection
            val attf = new AttributeSelection
            val eval = new WrapperSubsetEval
            val evalf = new WrapperSubsetEval
            val data = Seq(tr, ts) map Datasets.patterns2instancesId
            val dataf = Seq(trf, tsf, trfSemParecidos1) map Datasets.patterns2instancesId
            val sample = Datasets.patterns2instancesId(tr)
            val samplef = Datasets.patterns2instancesId(trf)
            eval.buildEvaluator(sample)
            evalf.buildEvaluator(samplef)
            //            val cla = new RandomForest
            //            val claf = new RandomForest
            val cla = new IBk
            val claf = new IBk
            //            cla.setDoNotCheckCapabilities(true)
            //            claf.setDoNotCheckCapabilities(true)
            //            cla.setNumTrees(20)
            //            claf.setNumTrees(20)
            cla.setKNN(5)
            claf.setKNN(5)
            eval.setFolds(10)
            eval.setClassifier(cla)
            eval.setThreshold(0.01)
            eval.setSeed(fold * run)
            evalf.setFolds(10)
            evalf.setClassifier(claf)
            evalf.setThreshold(0.01)
            evalf.setSeed(fold * run)
            att.setEvaluator(eval)
            attf.setEvaluator(evalf)
            val sea = new BestFirst()
            sea.setLookupCacheSize(10)
            //            sea.setSearchTermination(3)
            val seaf = new BestFirst()
            seaf.setLookupCacheSize(10)
            //            seaf.setSearchTermination(3)
            /*
            Searches the space of attribute subsets by greedy hillclimbing augmented with a backtracking facility.
            Setting the number of consecutive non-improving nodes allowed controls the level of backtracking done.
            Best first may start with the empty set of attributes and search forward, or start with the full set of
            attributes and search backward, or start at any point and search in both directions (by considering all
            possible single attribute additions and deletions at a given point).
             */
            sea.setStartSet("1")
            seaf.setStartSet("1")
            att.setSearch(sea)
            attf.setSearch(seaf)
            att.SelectAttributes(sample)
            attf.SelectAttributes(samplef)
            println(s"${att.selectedAttributes().toList} <- att.selectedAttributes()")
            println(s"${attf.selectedAttributes().toList} <- attf.selectedAttributes()")
            val seq = data map (x => Datasets.instances2patternsId(att.reduceDimensionality(x)).toVector)
            val seqf = dataf map (x => Datasets.instances2patternsId(attf.reduceDimensionality(x)).toVector)
            (seq(0), seqf(0), seq(1), seqf(1), seqf(2))
          } else (tr, trf, ts, tsf, trfSemParecidos1)

          leas(trfs) map { le =>
            val (trtestbags, tstestbags, m) = if (le.querFiltro) {
              val mo = le match {
                case NinteraELM(_, _) =>
                  val l = NinteraELM(seed)
                  //pega apenas a média dos exs. de cada base
                  val m0 = l.batchBuild(trfSemParecidos1fs).asInstanceOf[ELMModel] //foi melhor filtrar: 41,7 > 36,9
                val L = l.LForMeta(m0, LOO = true)
                  val m = l.batchBuild(trffs).asInstanceOf[ELMModel] //41,7 > 39,3
                  l.fullBuildForMeta(L, m)
                case SVMLibRBF(_) => SVMLibRBF(seed).build(trffs) //SVM fica um pouco mais rápida sem exemplos redundantes, mas 42,5 > 33,1
                case _ => le.build(trffs)
              }
              (trffs.groupBy(x => x.id), tsffs.groupBy(x => x.id), mo)
            } else {
              val mo = le match {
                case RF(_, n, _, _) => RF(seed, n).build(trfs)
                case _ => le.build(trfs)
              }
              (trfs.groupBy(x => x.id), tsfs.groupBy(x => x.id), mo)
            }
            val tr_ts = Vector(trtestbags, tstestbags) map { bags =>
              val resPorClasse = mutable.Queue[(String, String, Double)]()
              bags.map(_._2) foreach { xbag =>
                // statisticas p/ n>1 vão sair erradas com esse criterio abaixo
                val metaclass = xbag.head.nominalLabel
                val lab = m.predict(xbag.head).toInt
                val re = if (xbag.map(_.label).contains(lab)) 1d else 0d
                resPorClasse += ((metaclass, xbag.head.classAttribute.value(lab), re))
              }
              resPorClasse
            }
            Resultado(le.limpa, tr_ts.head, tr_ts(1))
          }
        }
      }
    }
  }

  def mapZipper(map1: Map[String, Int], map2: Map[String, Int]) = {
    for (key <- map1.keys ++ map2.keys)
      yield (key, map1.getOrElse(key, 0), map2.getOrElse(key, 0))
  }
}

/**
 * Cada entrada nas listas corresponde a um resultado(acc) ou uma combinação ocorrida de ranking(spearman).
 * Pode haver duplicados.
 */
case class Resultado(metalearner: String, valsTr: mutable.Queue[(String, String, Double)], valsTs: mutable.Queue[(String, String, Double)]) {
  val tottr = valsTr.size
  val totts = valsTs.size
  lazy val histTr = pretty(valsTr.groupBy(_._1))
  lazy val histTs = pretty(valsTs.groupBy(_._1))
  lazy val histTrPred = pretty(valsTr.groupBy(_._2))
  lazy val histTsPred = pretty(valsTs.groupBy(_._2))
  lazy val (accTr, accTs) = valsTr.map(_._3).sum / tottr -> valsTs.map(_._3).sum / totts

  def ++(that: Resultado) = if (that.metalearner != metalearner) ???
  else Resultado(metalearner, that.valsTr ++ valsTr, that.valsTs ++ valsTs)

  def pretty(s: Map[String, mutable.Queue[(String, String, Double)]]) = s.toList.sortBy(_._2.size).reverseMap(x => x._2.size + s" <- " + x._1)
}